{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d14a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from gensim.models import KeyedVectors\n",
    "from TorchCRF import CRF\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864caa7",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de67aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset JNLPBA depuis: ./datasets/JNLPBA\n",
      "- sentences: 50421 phrases\n",
      "- Classes: ['B-DNA', 'I-DNA', 'B-cell_line', 'I-cell_line', 'B-protein', 'I-protein', 'B-cell_type', 'I-cell_type', 'B-RNA', 'I-RNA', 'O']\n",
      "Chargement du dataset NCBI depuis: ./datasets/NCBI-Corpus/\n",
      "Documents chargés: 793\n",
      "Exemple d'entités dans le premier document: 2\n",
      "JNLPBA: 50421\n",
      "NCBI: 7526\n"
     ]
    }
   ],
   "source": [
    "from utils.fonctions import (\n",
    "    load_jnlpba_dataset,\n",
    "    load_ncbi_dataset,\n",
    "    prepare_ncbi_for_ner\n",
    ")\n",
    "\n",
    "jnlpba_data, _ = load_jnlpba_dataset(\"./datasets/JNLPBA\")\n",
    "\n",
    "ncbi_raw = load_ncbi_dataset(\"./datasets/NCBI-Corpus/\")\n",
    "ncbi_data = prepare_ncbi_for_ner(ncbi_raw)\n",
    "\n",
    "print(\"JNLPBA:\", len(jnlpba_data))\n",
    "print(\"NCBI:\", len(ncbi_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecf1bb",
   "metadata": {},
   "source": [
    "## NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53261ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    out = []\n",
    "    for sent in dataset:\n",
    "        s = []\n",
    "        for w, l in sent:\n",
    "            w = w.strip() if w.strip() else \"<UNK>\"\n",
    "            l = l.strip()\n",
    "            s.append((w, l))\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "jnlpba_data = normalize_dataset(jnlpba_data)\n",
    "ncbi_data   = normalize_dataset(ncbi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ee977",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bca5e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 57947\n"
     ]
    }
   ],
   "source": [
    "all_data = jnlpba_data + ncbi_data\n",
    "random.shuffle(all_data)\n",
    "\n",
    "print(\"Total sentences:\", len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d1811",
   "metadata": {},
   "source": [
    "## VOCABULARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4022ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocab: 26994\n",
      "Char vocab: 88\n",
      "Labels: {'<PAD>': 0, 'B-CompositeMention': 1, 'B-DNA': 2, 'B-DiseaseClass': 3, 'B-Modifier': 4, 'B-RNA': 5, 'B-SpecificDisease': 6, 'B-cell_line': 7, 'B-cell_type': 8, 'B-protein': 9, 'I-CompositeMention': 10, 'I-DNA': 11, 'I-DiseaseClass': 12, 'I-Modifier': 13, 'I-RNA': 14, 'I-SpecificDisease': 15, 'I-cell_line': 16, 'I-cell_type': 17, 'I-protein': 18, 'O': 19}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_word_vocab(data):\n",
    "    counter = Counter(w for sent in data for w, _ in sent)\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for w in counter:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def build_char_vocab(data):\n",
    "    chars = set(c for sent in data for w, _ in sent for c in w)\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for c in chars:\n",
    "        vocab[c] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def build_label_vocab(data):\n",
    "    labels = sorted(set(l for sent in data for _, l in sent))\n",
    "    vocab = {\"<PAD>\": 0}\n",
    "    for l in labels:\n",
    "        vocab[l] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "word_vocab  = build_word_vocab(all_data)\n",
    "char_vocab  = build_char_vocab(all_data)\n",
    "label_vocab = build_label_vocab(all_data)\n",
    "\n",
    "print(\"Word vocab:\", len(word_vocab))\n",
    "print(\"Char vocab:\", len(char_vocab))\n",
    "print(\"Labels:\", label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e1b25",
   "metadata": {},
   "source": [
    "## Rare Entity manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c69f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from collections import Counter\n",
    "\n",
    "# # Count the occurrences of each label\n",
    "# label_counts = Counter()\n",
    "# for sent in all_data:  # your encoded dataset\n",
    "#     label_counts.update(sent[\"labels\"])\n",
    "\n",
    "# # Compute inverse frequency weights\n",
    "# total = sum(label_counts.values())\n",
    "# class_weights = [total / (label_counts[i] + 1e-8) for i in range(len(label_vocab))]\n",
    "# class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# # Use with CrossEntropyLoss\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129a325",
   "metadata": {},
   "source": [
    "### Class-specific sampling / oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80dcc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_labels = [\"B-SpecificDisease\", \"B-CompositeMention\", \"B-DiseaseClass\"]\n",
    "rare_indices = [i for i, s in enumerate(all_data)\n",
    "                if any(lbl in rare_labels for _, lbl in s)]\n",
    "\n",
    "# Oversample rare sentences\n",
    "oversample_dataset = all_data + [all_data[i] for i in rare_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cce3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# # Example weights: inverse frequency of sentences containing rare entities\n",
    "# sample_weights = [2.0 if i in rare_indices else 1.0 for i in range(len(oversample_dataset))]\n",
    "# sampler = WeightedRandomSampler(sample_weights, num_samples=len(oversample_dataset), replacement=True)\n",
    "\n",
    "# train_loader = DataLoader(oversample_dataset, batch_size=32, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d28411",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438d6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "entity_dict = defaultdict(list)\n",
    "for sentence in all_data:\n",
    "    for token, label in sentence:\n",
    "        if label.startswith(\"B-\"):\n",
    "            entity_dict[label].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a14f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_entity(sentence, entity_dict):\n",
    "    new_sentence = []\n",
    "    for token, label in sentence:\n",
    "        if label.startswith(\"B-\") and label in entity_dict and entity_dict[label]:\n",
    "            # randomly replace with another entity of the same type\n",
    "            token = random.choice(entity_dict[label])\n",
    "        new_sentence.append((token, label))\n",
    "    return new_sentence\n",
    "\n",
    "data_augmented = [augment_entity(s, entity_dict) for s in all_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1588de9",
   "metadata": {},
   "source": [
    "## ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e97daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHAR_LEN = 30\n",
    "\n",
    "def encode(sent):\n",
    "    words, chars, labels = [], [], []\n",
    "    for w, l in sent:\n",
    "        words.append(word_vocab.get(w, word_vocab[\"<UNK>\"]))\n",
    "\n",
    "        c = [char_vocab.get(ch, char_vocab[\"<UNK>\"]) for ch in w][:MAX_CHAR_LEN]\n",
    "        c += [char_vocab[\"<PAD>\"]] * (MAX_CHAR_LEN - len(c))\n",
    "        chars.append(c)\n",
    "\n",
    "        labels.append(label_vocab[l])\n",
    "\n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"chars\": chars,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed2e1e",
   "metadata": {},
   "source": [
    "## Encode + Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1eb33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 52152\n",
      "Val: 5795\n"
     ]
    }
   ],
   "source": [
    "encoded_all = [encode(s) for s in data_augmented]\n",
    "random.shuffle(encoded_all)\n",
    "\n",
    "split = int(0.9 * len(encoded_all))\n",
    "train_data = encoded_all[:split]\n",
    "val_data   = encoded_all[split:]\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Val:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1a521",
   "metadata": {},
   "source": [
    "### Compute rare indices and sample weights only for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322d4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "rare_labels = [\"B-SpecificDisease\", \"B-CompositeMention\", \"B-DiseaseClass\"]\n",
    "label_vocab_inv = {v: k for k, v in label_vocab.items()}\n",
    "\n",
    "\n",
    "rare_indices_train = [\n",
    "    i for i, s in enumerate(train_data)\n",
    "    if any(label_vocab_inv[lbl] in rare_labels for lbl in s[\"labels\"])\n",
    "]\n",
    "\n",
    "sample_weights = [2.0 if i in rare_indices_train else 1.0 for i in range(len(train_data))]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_data), replacement=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27520de",
   "metadata": {},
   "source": [
    "## DATASET & COLLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194dad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data): self.data = data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i): return self.data[i]\n",
    "\n",
    "def collate(batch):\n",
    "    max_len = max(len(x[\"words\"]) for x in batch)\n",
    "    pad = lambda x, v: x + [v] * (max_len - len(x))\n",
    "    words = [pad(x[\"words\"], 0) for x in batch]\n",
    "    labels = [pad(x[\"labels\"], 0) for x in batch]\n",
    "    chars = [x[\"chars\"] + [[0]*MAX_CHAR_LEN]*(max_len-len(x[\"chars\"])) for x in batch]\n",
    "    mask = [[1]*len(x[\"words\"]) + [0]*(max_len-len(x[\"words\"])) for x in batch]\n",
    "\n",
    "    return (\n",
    "        torch.tensor(words),\n",
    "        torch.tensor(chars),\n",
    "        torch.tensor(labels),\n",
    "        torch.tensor(mask, dtype=torch.bool)\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(NERDataset(train_data), batch_size=32, sampler=sampler, collate_fn=collate)\n",
    "val_loader   = DataLoader(NERDataset(val_data), 32, False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6db61",
   "metadata": {},
   "source": [
    "## Word Embedding with  BioWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbfec807",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = KeyedVectors.load(\"./embeddings/biowordvec.gensim\", mmap=\"r\")\n",
    "EMB_DIM = kv.vector_size\n",
    "\n",
    "emb_matrix = np.random.normal(0, 0.6, (len(word_vocab), EMB_DIM))\n",
    "for w, i in word_vocab.items():\n",
    "    if w in kv:\n",
    "        emb_matrix[i] = kv[w]\n",
    "\n",
    "word_embeddings = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(emb_matrix, dtype=torch.float),\n",
    "    freeze=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a11dcc",
   "metadata": {},
   "source": [
    "## Char Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d129fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(char_vocab), 30, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(30, 50, k, padding=k//2) for k in (3,4,5)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, L = x.shape\n",
    "        x = self.emb(x).view(B*T, L, -1).transpose(1,2)\n",
    "        feats = [torch.max(torch.relu(conv(x)), 2)[0] for conv in self.convs]\n",
    "        return torch.cat(feats, 1).view(B, T, -1)\n",
    "\n",
    "class CharBiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(char_vocab), 30, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(30, 50, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, L = x.shape\n",
    "        x = self.emb(x).view(B*T, L, -1)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return torch.cat([h[0], h[1]], 1).view(B, T, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be470d27",
   "metadata": {},
   "source": [
    "## MANHATTAN ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355c28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManhattanAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, h, mask):\n",
    "        B, T, D = h.shape\n",
    "        hi = h.unsqueeze(2).expand(B, T, T, D)\n",
    "        hj = h.unsqueeze(1).expand(B, T, T, D)\n",
    "        dist = torch.abs(hi - hj).sum(-1)\n",
    "        score = -self.W(hj).squeeze(-1) * dist\n",
    "        score = score.masked_fill(~mask.unsqueeze(1), -1e9)\n",
    "        alpha = torch.softmax(score, -1)\n",
    "        ctx = torch.matmul(alpha, h)\n",
    "        return torch.cat([h, ctx], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78615d91",
   "metadata": {},
   "source": [
    "## FULL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6debd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioNER(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.word_emb = word_embeddings\n",
    "        self.char_cnn = CharCNN()\n",
    "        self.char_lstm = CharBiLSTM()\n",
    "\n",
    "        concat_dim = EMB_DIM + 150 + 100\n",
    "        self.embed_fc = nn.Linear(concat_dim, 200)\n",
    "\n",
    "        self.bilstm = nn.LSTM(200, 256, bidirectional=True, batch_first=True)\n",
    "        self.attn = ManhattanAttention(512)\n",
    "        self.fc = nn.Linear(1024, len(label_vocab))\n",
    "        self.crf = CRF(len(label_vocab), batch_first=True)\n",
    "\n",
    "    def forward(self, w, c, mask, labels=None):\n",
    "        we = self.word_emb(w)\n",
    "        ce1 = self.char_cnn(c)\n",
    "        ce2 = self.char_lstm(c)\n",
    "\n",
    "        x = torch.cat([we, ce1, ce2], -1)\n",
    "        x = self.embed_fc(x)\n",
    "\n",
    "        h, _ = self.bilstm(x)\n",
    "        h = self.attn(h, mask)\n",
    "        emissions = self.fc(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            return -self.crf(emissions, labels, mask)\n",
    "        return self.crf.decode(emissions, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1846454",
   "metadata": {},
   "source": [
    "## TRAINING & METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab93cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
    "    acc = sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "    return acc, p, r, f\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss, yt, yp = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for w, c, l, m in loader:\n",
    "            w, c, l, m = w.to(device), c.to(device), l.to(device), m.to(device)\n",
    "            loss += model(w, c, m, l).item()\n",
    "            preds = model(w, c, m)\n",
    "            for p, g, mask in zip(preds, l, m):\n",
    "                L = mask.sum().item()\n",
    "                yp.extend(p[:L])\n",
    "                yt.extend(g[:L].tolist())\n",
    "    acc, p, r, f = metrics(yt, yp)\n",
    "    return loss/len(loader), acc, p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fda9ca",
   "metadata": {},
   "source": [
    "## TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be95a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train L:312.0564 || Val L:175.5166 Acc:0.9243\n",
      "Epoch 02 | Train L:147.3941 || Val L:132.0257 Acc:0.9391\n",
      "Epoch 03 | Train L:110.6399 || Val L:115.4803 Acc:0.9426\n",
      "Epoch 04 | Train L:87.0613 || Val L:91.4574 Acc:0.9536\n",
      "Epoch 05 | Train L:71.6095 || Val L:79.8151 Acc:0.9579\n",
      "Epoch 06 | Train L:58.9517 || Val L:70.9379 Acc:0.9608\n",
      "Epoch 07 | Train L:49.2656 || Val L:63.9244 Acc:0.9638\n",
      "Epoch 08 | Train L:42.0047 || Val L:59.0211 Acc:0.9667\n",
      "Epoch 09 | Train L:34.8477 || Val L:54.8994 Acc:0.9688\n",
      "Epoch 10 | Train L:28.8983 || Val L:52.3155 Acc:0.9710\n",
      "Epoch 11 | Train L:24.3814 || Val L:51.9269 Acc:0.9710\n",
      "Epoch 12 | Train L:20.1358 || Val L:47.9246 Acc:0.9741\n",
      "Epoch 13 | Train L:16.6448 || Val L:45.0855 Acc:0.9761\n",
      "Epoch 14 | Train L:13.6066 || Val L:44.4027 Acc:0.9759\n",
      "Epoch 15 | Train L:11.1613 || Val L:43.8076 Acc:0.9772\n",
      "Epoch 16 | Train L:9.0042 || Val L:48.0408 Acc:0.9758\n",
      "Epoch 17 | Train L:7.4325 || Val L:44.1408 Acc:0.9780\n",
      "Epoch 18 | Train L:6.0605 || Val L:43.4781 Acc:0.9793\n",
      "Epoch 19 | Train L:5.1658 || Val L:41.7715 Acc:0.9796\n",
      "Epoch 20 | Train L:3.8769 || Val L:46.8490 Acc:0.9792\n",
      "Epoch 21 | Train L:3.3055 || Val L:43.5704 Acc:0.9804\n",
      "Epoch 22 | Train L:2.8243 || Val L:46.9162 Acc:0.9797\n",
      "Epoch 23 | Train L:2.7164 || Val L:45.7545 Acc:0.9803\n",
      "Epoch 24 | Train L:1.6796 || Val L:47.2559 Acc:0.9798\n",
      "Epoch 25 | Train L:2.2434 || Val L:45.9123 Acc:0.9807\n",
      "Epoch 26 | Train L:1.3507 || Val L:46.1138 Acc:0.9812\n",
      "Epoch 27 | Train L:1.6132 || Val L:46.2094 Acc:0.9818\n",
      "Epoch 28 | Train L:1.0732 || Val L:46.9454 Acc:0.9812\n",
      "Epoch 29 | Train L:1.3652 || Val L:49.1313 Acc:0.9802\n",
      "Epoch 30 | Train L:1.3730 || Val L:50.6391 Acc:0.9809\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "model = BioNER().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "best_val_acc = 0\n",
    "wait = 0\n",
    "patience = 5\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- training ----\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for w, c, l, m in train_loader:\n",
    "        w, c, l, m = w.to(device), c.to(device), l.to(device), m.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = model(w, c, m, l)  # forward with labels\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    tr_loss /= n_batches  # average training loss\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    yt_val, yp_val = [], []\n",
    "    n_val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w, c, l, m in val_loader:\n",
    "            w, c, l, m = w.to(device), c.to(device), l.to(device), m.to(device)\n",
    "            \n",
    "            # loss computation\n",
    "            loss = model(w, c, m, l)\n",
    "            val_loss += loss.item()\n",
    "            n_val_batches += 1\n",
    "\n",
    "            # predictions\n",
    "            preds = model(w, c, m)\n",
    "            for p, g, mask in zip(preds, l, m):\n",
    "                L = mask.sum().item()\n",
    "                yp_val.extend(p[:L])\n",
    "                yt_val.extend(g[:L].tolist())\n",
    "\n",
    "    val_loss /= n_val_batches\n",
    "    val_acc = (torch.tensor(yp_val)[:len(yt_val)] == torch.tensor(yt_val)).float().mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train L:{tr_loss:.4f} || Val L:{val_loss:.4f} Acc:{val_acc:.4f}\")\n",
    "\n",
    "    # ---- early stopping ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_bioner.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96a31955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-level Precision: 0.9411\n",
      "Entity-level Recall:    0.9387\n",
      "Entity-level F1:        0.9399\n",
      "\n",
      "Per-entity metrics:\n",
      "DNA          P:0.901 R:0.883 F1:0.892\n",
      "protein      P:0.948 R:0.924 F1:0.936\n",
      "cell_type    P:0.888 R:0.912 F1:0.900\n",
      "cell_line    P:0.913 R:0.833 F1:0.871\n",
      "SpecificDisease P:0.716 R:0.685 F1:0.700\n",
      "RNA          P:0.900 R:0.892 F1:0.896\n",
      "DiseaseClass P:0.511 R:0.442 F1:0.474\n",
      "Modifier     P:0.685 R:0.570 F1:0.622\n",
      "CompositeMention P:0.571 R:0.308 F1:0.400\n"
     ]
    }
   ],
   "source": [
    "def bio_to_spans(labels, id2label):\n",
    "    \"\"\"\n",
    "    Convert a BIO tag sequence into entity spans.\n",
    "    Returns: list of (entity_type, start, end)\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    start = None\n",
    "    ent_type = None\n",
    "\n",
    "    for i, lab_id in enumerate(labels):\n",
    "        label = id2label[lab_id]\n",
    "\n",
    "        if label == \"O\":\n",
    "            if ent_type is not None:\n",
    "                spans.append((ent_type, start, i - 1))\n",
    "                ent_type = None\n",
    "            continue\n",
    "\n",
    "        tag, typ = label.split(\"-\", 1)\n",
    "\n",
    "        if tag == \"B\":\n",
    "            if ent_type is not None:\n",
    "                spans.append((ent_type, start, i - 1))\n",
    "            ent_type = typ\n",
    "            start = i\n",
    "\n",
    "        elif tag == \"I\":\n",
    "            if ent_type != typ:\n",
    "                # BIO violation → start new entity\n",
    "                if ent_type is not None:\n",
    "                    spans.append((ent_type, start, i - 1))\n",
    "                ent_type = typ\n",
    "                start = i\n",
    "\n",
    "    if ent_type is not None:\n",
    "        spans.append((ent_type, start, len(labels) - 1))\n",
    "\n",
    "    return spans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def entity_level_metrics(y_true, y_pred, id2label):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: lists of label-id sequences (one per sentence)\n",
    "    \"\"\"\n",
    "    gold_spans = []\n",
    "    pred_spans = []\n",
    "\n",
    "    for gt, pr in zip(y_true, y_pred):\n",
    "        gold_spans.extend(bio_to_spans(gt, id2label))\n",
    "        pred_spans.extend(bio_to_spans(pr, id2label))\n",
    "\n",
    "    gold_set = set(gold_spans)\n",
    "    pred_set = set(pred_spans)\n",
    "\n",
    "    tp = len(gold_set & pred_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def entity_metrics_by_type(y_true, y_pred, id2label):\n",
    "    gold_by_type = Counter()\n",
    "    pred_by_type = Counter()\n",
    "    correct_by_type = Counter()\n",
    "\n",
    "    for gt, pr in zip(y_true, y_pred):\n",
    "        gold_spans = bio_to_spans(gt, id2label)\n",
    "        pred_spans = bio_to_spans(pr, id2label)\n",
    "\n",
    "        gold_set = set(gold_spans)\n",
    "        pred_set = set(pred_spans)\n",
    "\n",
    "        for ent, _, _ in gold_set:\n",
    "            gold_by_type[ent] += 1\n",
    "        for ent, _, _ in pred_set:\n",
    "            pred_by_type[ent] += 1\n",
    "        for ent, _, _ in gold_set & pred_set:\n",
    "            correct_by_type[ent] += 1\n",
    "\n",
    "    results = {}\n",
    "    for ent in gold_by_type:\n",
    "        p = correct_by_type[ent] / (pred_by_type[ent] + 1e-8)\n",
    "        r = correct_by_type[ent] / (gold_by_type[ent] + 1e-8)\n",
    "        f = 2 * p * r / (p + r + 1e-8)\n",
    "        results[ent] = (p, r, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# ----- 1. Load your label mapping -----\n",
    "id2label = {v: k for k, v in label_vocab.items()}  # assuming you have label_vocab dict\n",
    "\n",
    "# ----- 2. Load your trained model -----\n",
    "model = BioNER().to(device)\n",
    "model.load_state_dict(torch.load(\"best_bioner.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----- 3. Define evaluation using entity-level metrics -----\n",
    "def evaluate_entity_level(model, loader, id2label):\n",
    "    all_gold, all_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w, c, l, m in loader:\n",
    "            w, c, l, m = w.to(device), c.to(device), l.to(device), m.to(device)\n",
    "            preds = model(w, c, m)\n",
    "\n",
    "            for p, g, mask in zip(preds, l, m):\n",
    "                L = mask.sum().item()\n",
    "                all_pred.append(p[:L])\n",
    "                all_gold.append(g[:L].tolist())\n",
    "\n",
    "    # Use entity-level metric function\n",
    "    p, r, f = entity_level_metrics(all_gold, all_pred, id2label)\n",
    "    per_type = entity_metrics_by_type(all_gold, all_pred, id2label)\n",
    "\n",
    "    return p, r, f, per_type\n",
    "\n",
    "# ----- 4. Run evaluation -----\n",
    "precision, recall, f1, per_entity = evaluate_entity_level(model, val_loader, id2label)\n",
    "\n",
    "print(f\"Entity-level Precision: {precision:.4f}\")\n",
    "print(f\"Entity-level Recall:    {recall:.4f}\")\n",
    "print(f\"Entity-level F1:        {f1:.4f}\\n\")\n",
    "\n",
    "print(\"Per-entity metrics:\")\n",
    "for ent, (p, r, f) in per_entity.items():\n",
    "    print(f\"{ent:12s} P:{p:.3f} R:{r:.3f} F1:{f:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
